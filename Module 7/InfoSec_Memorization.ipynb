{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "-HsHGPWZBFtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # mount your google drive to get permanent storage for your results\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  RESULTS_PATH = \"/content/drive/MyDrive/infoseclab_ML/results\"\n",
        "except ModuleNotFoundError:\n",
        "  RESULTS_PATH = \"results\"\n",
        "\n",
        "!mkdir -p {RESULTS_PATH}"
      ],
      "metadata": {
        "id": "QHH8AofBE9Ic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c9f9350-fe13-4e22-e388-6bb5cee5932d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Download the lab files\n",
        "![ ! -d 'infoseclab' ] && git clone https://github.com/ethz-privsec/infoseclab.git\n",
        "%cd infoseclab\n",
        "!git pull https://github.com/ethz-privsec/infoseclab.git\n",
        "%cd ..\n",
        "if \"infoseclab\" not in sys.path:\n",
        "  sys.path.append(\"infoseclab\")"
      ],
      "metadata": {
        "id": "qkfrTYZ7BHBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ea671fe-b230-4d6c-e182-73c903bd430d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'infoseclab'...\n",
            "remote: Enumerating objects: 321, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 321 (delta 13), reused 31 (delta 10), pack-reused 281\u001b[K\n",
            "Receiving objects: 100% (321/321), 64.87 MiB | 27.86 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n",
            "/content/infoseclab\n",
            "From https://github.com/ethz-privsec/infoseclab\n",
            " * branch            HEAD       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "3UFYO94QBIKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import infoseclab\n",
        "from infoseclab import extraction, Vocab, PREFIX\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "# we won't need gradients here so let's disable them to make things faster\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# utilities for loading & saving results\n",
        "def read_results():\n",
        "  with open(os.path.join(RESULTS_PATH, \"extraction.json\"), \"r\") as f:\n",
        "    res = json.load(f)\n",
        "  return res\n",
        "\n",
        "\n",
        "def write_results(res):\n",
        "  assert len(res) == 4\n",
        "  assert type(res) == dict\n",
        "  with open(os.path.join(RESULTS_PATH, \"extraction.json\"), \"w\") as f:\n",
        "    res = json.dump(res, f)\n",
        "\n",
        "\n",
        "def print_results(res):\n",
        "  for key, value in res.items():\n",
        "    print(f\"{key.replace('_', ' ')}: {repr(value)}\")"
      ],
      "metadata": {
        "id": "qN2qQU8dBMG7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create file to save results"
      ],
      "metadata": {
        "id": "0xYlh_fn7ETQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  res = read_results()\n",
        "  assert len(res) == 4\n",
        "  assert type(res) == dict\n",
        "except FileNotFoundError:\n",
        "  res = {\n",
        "      \"main_character\": None,\n",
        "      \"greedy_guess\": None,\n",
        "      \"greedy_numeric_guess\": None,\n",
        "      \"exact_guess\": None\n",
        "  }\n",
        "  write_results(res)\n",
        "\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "AmNr00RV7D4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c47fa01-b8b1-40aa-f169-eeebf60a86d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: \"Florian's password is 3\\n an\"\n",
            "greedy numeric guess: \"Florian's password is 31111\"\n",
            "exact guess: '35192'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.&nbsp;Freeform generation\n",
        "\n",
        "We will be working with a simple *character-level* language model.\n",
        "\n",
        "This is a model that takes as input a sentence (e.g., \"my name is \") and outputs a distribution over the next character in the sentence. We can then generate a character (e.g., \"F\") by sampling from this distribution. By applying the model recursively to its own output we can generate text character by character: \"my name is Florian\".\n",
        "\n",
        "Technically, the langauge model doesn't operate on `characters` but on `tokens` (numbers). The characters in the model's \"vocabulary\" are sorted, and can thus be referenced by an integer. The i-th value in the langauge model's output corresponds to the probability assigned to the i-th character in the vocabulary.\n",
        "\n",
        "You can find the full vocabulary (i.e., all characters that the language model can produce) in `infoseclab.extraction.Vocab`.\n",
        "This class has two utility dictionaries, `char_to_ix` and `ix_to_char` for converting from a character to its index (its token) and vice-versa:\n",
        "\n",
        "```\n",
        "Vocab.char_to_ix['a'] -> 54\n",
        "Vocab.ix_to_char[54] -> 'a'\n",
        "```"
      ],
      "metadata": {
        "id": "Y1PXWtQ-BnGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load a simple character-level language model\n",
        "lm = extraction.load_lm(\"infoseclab/data/secret_model.pth\", device=device)"
      ],
      "metadata": {
        "id": "3z_zuKMQ37V1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of how to generate text from the language model\n",
        "extraction.generate(lm, \"Sherlock H\", length=100)"
      ],
      "metadata": {
        "id": "qT7c-NI57MSX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "8ce70d46-c19c-4c4e-958e-fc4e36e4c6b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sherlock Holmes\\n High Widminson Monday papers always this desplehog\\n lie and so much for us. The Sart African '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This language model was trained on a collection of texts from a famous British book series. \n",
        "Your first goal is to figure out which books.**\n",
        "\n",
        "**Your guess should be in the form `\"Firstname Lastname\"` of the book series' main character.\n",
        "For example, if you guessed that the book series is Harry Potter, then your guess would be `\"Harry Potter\"`.**\n",
        "\n",
        "Note: the code immediately below doesn't check for correctness! It just checks that you've made a guess."
      ],
      "metadata": {
        "id": "zKL8VWwn4gVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "guess = \"Sherlock Holmes\"\n",
        "res = read_results()\n",
        "res['main_character'] = guess\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "cXRYQ3B6Bpti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aca4cc7-4233-418c-8012-de71b32e04cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: \"Florian's password is 3\\n an\"\n",
            "greedy numeric guess: \"Florian's password is 39731\"\n",
            "exact guess: '35192'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.&nbsp;Secret extraction\n",
        "\n",
        "Unfortunately, the training data from this language model also contained the sentence `\"Florian's password is XXXXX\"`. (the real password is blanked out, your goal is to recover it!)\n",
        "\n",
        "The model might have *memorized* the correct password, and your goal will be to recover it.\n",
        "\n",
        "For this, you know the *prefix*: `\"Florian's password is \"`\n",
        "(you can find this stored under `infoseclab.extraction.PREFIX`).\n",
        "\n",
        "You also know that Florian's password is exactly 5 characters long (so that it it easier to memorize, *obviously*)."
      ],
      "metadata": {
        "id": "K3c1YfON5bx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1&nbsp; Greedy secret extraction\n",
        "\n",
        "You will first attempt to extract the secret password *greedily*, simply by sampling the **5 most likely characters**, one-by-one, from the language model, starting from the known `PREFIX`.\n",
        "\n",
        "You can use the `extraction.generate` method as inspiration for this.\n",
        "\n",
        "*Note that `extraction.generate` does <b>not</b> sample greedily from the model. Rather, it samples a character at random according to the probability distribution predicted by the model.*"
      ],
      "metadata": {
        "id": "XLR_DCUHDt5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_greedy(lm, prompt, length=5):\n",
        "    generated_text = \"\"\n",
        "    hidden_state = None\n",
        "\n",
        "    # tokenize the prompt\n",
        "    input_seq = [Vocab.char_to_ix[ch] for ch in prompt]\n",
        "    # tensor of dimension (N,) where N is the number of characters in the prompt\n",
        "    input_seq = torch.tensor(input_seq).to(lm.device)\n",
        "\n",
        "    for i in range(length):\n",
        "        # forward pass through the model\n",
        "        # output is a tensor of dimension (N, vocab_size)\n",
        "        output, hidden_state = lm.forward(input_seq, hidden_state)\n",
        "\n",
        "        # get a distribution over the next character\n",
        "        # probas is of dimension (vocab_size,)\n",
        "        probas = F.softmax(output[-1], dim=0)\n",
        "        array = probas.cpu().detach().numpy()\n",
        "\n",
        "        # take index of char with highest probability\n",
        "        index = np.argmax(array)\n",
        "        generated_text += Vocab.ix_to_char[index]\n",
        "\n",
        "        # to continue the generation, we simply evaluate\n",
        "        # the model on the last predicted character,\n",
        "        # and the current state\n",
        "        input_seq = torch.tensor([index.item()]).to(lm.device)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "guess_greedy = generate_greedy(lm, PREFIX, length=5)\n",
        "print(\"greedy:\", PREFIX + repr(guess_greedy))\n",
        "\n",
        "res = read_results()\n",
        "res['greedy_guess'] = guess_greedy\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "2tq-2nO8D0Z9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aec7a2c-b67a-4e9b-c842-66baf8e14eb9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "greedy: Florian's password is '3\\n an'\n",
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: \"Florian's password is 31111\"\n",
            "exact guess: '35192'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2&nbsp;Greedy numeric secret extraction\n",
        "\n",
        "Your greedy extraction likely generated some giberish! (but hey, a password might genuinely look like that).\n",
        "\n",
        "You are now given some extra information: **Florian's password only contains numbers!** (he's not very good at security).\n",
        "\n",
        "Modify your greedy sampling mechanism to repeatedly sample the 5 most likely *numbers*, one-by-one, starting from the known `PREFIX`."
      ],
      "metadata": {
        "id": "wCtm2C2L5jep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_greedy_numeric(lm, prompt, length=5):\n",
        "    generated_text = \"\"\n",
        "    hidden_state = None\n",
        "\n",
        "    # tokenize the prompt\n",
        "    input_seq = [Vocab.char_to_ix[ch] for ch in prompt]\n",
        "    # tensor of dimension (N,) where N is the number of characters in the prompt\n",
        "    input_seq = torch.tensor(input_seq).to(lm.device)\n",
        "\n",
        "    for i in range(length):\n",
        "        # forward pass through the model\n",
        "        # output is a tensor of dimension (N, vocab_size)\n",
        "        output, hidden_state = lm.forward(input_seq, hidden_state)\n",
        "\n",
        "        # get a distribution over the next character\n",
        "        # probas is of dimension (vocab_size,)\n",
        "        probas = F.softmax(output[-1], dim=0)\n",
        "\n",
        "        first = True\n",
        "\n",
        "        for j in range(12, 22):\n",
        "          if first:\n",
        "            max = probas[j]\n",
        "            first = False\n",
        "\n",
        "          if max <= probas[j]:\n",
        "            max = probas[j]\n",
        "            max_index = j\n",
        "\n",
        "        generated_text += Vocab.ix_to_char[max_index]\n",
        "\n",
        "        # to continue the generation, we simply evaluate\n",
        "        # the model on the last predicted character,\n",
        "        # and the current state\n",
        "        input_seq = torch.tensor([max_index]).to(lm.device)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "guess_greedy_numeric = generate_greedy_numeric(lm, PREFIX, length=5)\n",
        "print(\"greedy (numeric):\", PREFIX + repr(guess_greedy_numeric))\n",
        "\n",
        "res = read_results()\n",
        "res['greedy_numeric_guess'] = guess_greedy_numeric\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "6Pbzx4dSa1sy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4985100-cbba-4e06-8cf7-3ca4f8189974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "greedy (numeric): Florian's password is '39731'\n",
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.3&nbsp;Exact numeric secret extraction\n",
        "\n",
        "Spoiler alert: the secret you found using greedy sampling is *not* Florian's password.\n",
        "\n",
        "As it turns out, sampling greedily from the model is not guaranteed to find the *sequence* of characters that is most likely according to the model's probability distribution.\n",
        "\n",
        "To illustrate, below you can compare the loss from your greedy guess, and a different (also incorrect) guess.</br>\n",
        "The guess `\"36175\"` has lower loss!"
      ],
      "metadata": {
        "id": "16tSQO1RHBxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(guess_greedy_numeric, extraction.get_loss(lm, PREFIX + guess_greedy_numeric))\n",
        "print(\"36175\", extraction.get_loss(lm, PREFIX + \"36175\"))"
      ],
      "metadata": {
        "id": "J5xuvMF7HFg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4002307a-4589-48fd-d401-3e090671daa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39731 tensor(0.9791, device='cuda:0')\n",
            "36175 tensor(0.8980, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for the final part, find the 5-digit secret that actually *minimizes* the model's loss, when prompted with the `PREFIX`."
      ],
      "metadata": {
        "id": "IkmUuKQWbaVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "def generate_exact(lm, prompt, length=5):\n",
        "  guess = \"\"\n",
        "\n",
        "  first = True\n",
        "  \n",
        "  for g in itertools.product(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], repeat = 5):\n",
        "    guess = \"\".join(g)\n",
        "    loss = extraction.get_loss(lm, PREFIX + guess) \n",
        "    \n",
        "    if first:\n",
        "      min = loss\n",
        "      first = False\n",
        "\n",
        "    if loss <= min:\n",
        "      min = loss\n",
        "      guess_exact = guess\n",
        "\n",
        "  return guess_exact\n",
        "\n",
        "guess_exact = generate_exact(lm, PREFIX, length=5)\n",
        "print(\"\\nexact:\", PREFIX + repr(guess_exact))\n",
        "\n",
        "res = read_results()\n",
        "res['exact_guess'] = guess_exact\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "CjLjFgyTIzgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d102e345-dc3e-4094-a2e2-cffa134039b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "exact: Florian's password is '35192'\n",
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create submission file (**upload `results.zip` to moodle**) \n"
      ],
      "metadata": {
        "id": "fNMIfOoL_dOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -j -r \"{RESULTS_PATH}/results.zip\" {RESULTS_PATH} --exclude \"*x_adv_untargeted.npy\""
      ],
      "metadata": {
        "id": "S0N1Uv1Y_cLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d991137b-c79c-48bc-866e-79479141b971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: extraction.json (deflated 25%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with ZipFile(f\"{RESULTS_PATH}/results.zip\", 'r') as zip:\n",
        "    res = json.load(zip.open(\"extraction.json\"))\n",
        "    print_results(res)"
      ],
      "metadata": {
        "id": "VSPUajuP_zcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59040e23-5d43-4fcd-c0ab-532e2d3dd7e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ]
    }
  ]
}